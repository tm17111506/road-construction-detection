{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8326069d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nuscenes-devkit &> /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ec58f738",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nuimages import NuImages\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import multiprocessing\n",
    "from multiprocessing.dummy import Pool as ThreadPool\n",
    "from itertools import groupby\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bdb0328f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataroot = '/usr0/tma1/datasets/nuimages'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a2778ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating category id files...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "print('Creating category id files...')\n",
    "category_file = os.path.join(dataroot, 'category_id.json')\n",
    "if (not os.path.exists(category_file)):\n",
    "    fd = open(os.path.join(dataroot, 'v1.0-train/category.json'), 'r')\n",
    "    category = json.load(fd)\n",
    "    category_id = dict()\n",
    "    \n",
    "    for i, cat in enumerate(category):\n",
    "        category_id[cat['name']] = i\n",
    "    \n",
    "    fd2 = open(category_file, 'w+')\n",
    "    json.dump(category_id, fd2)\n",
    "    \n",
    "    fd.close()\n",
    "    fd2.close()\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf85f9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "category_id_file = os.path.join(dataroot, 'category_id.json')\n",
    "fd = open(category_id_file, 'r')\n",
    "category_id = json.load(fd)\n",
    "fd.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "7279633a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def object_ann_detectron_format(nuim, sample):\n",
    "    data = dict()\n",
    "    \n",
    "    # Get sample\n",
    "    sd = nuim.get('sample_data', sample['key_camera_token'])\n",
    "    data['file_name'] = os.path.join(dataroot, sd['filename'])\n",
    "    data['height'] = sd['height']\n",
    "    data['width'] = sd['width']\n",
    "    data['image_id'] = sample['key_camera_token']\n",
    "    \n",
    "    annotations = []\n",
    "    obj_anns, _ = nuim.list_anns(sample['token'], verbose=False)\n",
    "\n",
    "    for obj_ann_token in obj_anns:\n",
    "        ann = dict()\n",
    "        obj_ann = nuim.get('object_ann', obj_ann_token)\n",
    "        ann['bbox'] = obj_ann['bbox']\n",
    "        ann['bbox_mode'] = 0 # detectron2.structures.BoxMode.XYXY_ABS\n",
    "        ann_category = nuim.get('category', obj_ann['category_token'])\n",
    "        ann['category_id'] = category_id[ann_category['name']]\n",
    "\n",
    "        annotations.append(ann)\n",
    "    data['annotations'] = annotations\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072b5b9b",
   "metadata": {},
   "source": [
    "Save the annotations of each split into detectron input data format\n",
    "Test has no data because no annotations. Below is generated based on annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5da93134",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating detectron data for v1.0-train\n",
      "======\n",
      "Loading nuImages tables for version v1.0-train...\n",
      "Done loading in 0.000 seconds (lazy=True).\n",
      "======\n",
      "Loaded 67279 sample(s) in 1.015s,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/67279 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 872181 sample_data(s) in 43.912s,\n",
      "Loaded 557715 object_ann(s) in 25.733s,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 1/67279 [01:24<1575:08:48, 84.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 73755 surface_ann(s) in 13.788s,\n",
      "Loaded 25 category(s) in 0.016s,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67279/67279 [52:04<00:00, 21.53it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... done\n",
      "Generating detectron data for v1.0-val\n",
      "======\n",
      "Loading nuImages tables for version v1.0-val...\n",
      "Done loading in 0.000 seconds (lazy=True).\n",
      "======\n",
      "Loaded 16445 sample(s) in 0.067s,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/16445 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 213185 sample_data(s) in 1.132s,\n",
      "Loaded 136074 object_ann(s) in 0.888s,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 11/16445 [00:03<56:52,  4.82it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 17995 surface_ann(s) in 0.892s,\n",
      "Loaded 25 category(s) in 0.013s,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16445/16445 [02:52<00:00, 95.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... done\n",
      "Generating detectron data for v1.0-test\n",
      "======\n",
      "Loading nuImages tables for version v1.0-test...\n",
      "Done loading in 0.000 seconds (lazy=True).\n",
      "======\n",
      "Loaded 9752 sample(s) in 0.011s,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9752/9752 [00:00<00:00, 12820.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 126276 sample_data(s) in 0.661s,\n",
      "Loaded 0 object_ann(s) in 0.000s,\n",
      "Loaded 0 surface_ann(s) in 0.000s,\n",
      "Loaded 25 category(s) in 0.002s,\n",
      "... done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "splits = ['v1.0-train', 'v1.0-val', 'v1.0-test']\n",
    "for split in splits:\n",
    "    print(\"Generating detectron data for {}\".format(split))\n",
    "    split_data = []\n",
    "    nuim = NuImages(dataroot=dataroot, version=split, verbose=True, lazy=True)\n",
    "    \n",
    "    for s in tqdm(nuim.sample):\n",
    "        data = object_ann_detectron_format(nuim, s)\n",
    "        split_data.append(data)\n",
    "    \n",
    "#     grouped_anns = groupby(nuim.object_ann, lambda k: k['sample_data_token'])\n",
    "#     for k, v in tqdm(grouped_anns):\n",
    "#         sd_token = list(v)[0]['sample_data_token']\n",
    "#         object_anns = [o for o in nuim.object_ann if o['sample_data_token'] == sd_token]\n",
    "#         print(\"num objects\", len(object_anns))\n",
    "#         nuim.render_image(sd_token, annotation_type='all',\n",
    "#                           with_category=True, with_attributes=True, box_line_width=-1, render_scale=5)\n",
    "#         data = object_ann_detectron_format(nuim, k, v)\n",
    "#         split_data.append(data)\n",
    "#         break\n",
    "#     break\n",
    "    \n",
    "    filename = os.path.join(os.path.join(dataroot, 'detectron_data'), split + '_detectron.json')\n",
    "    fd = open(filename, 'w+')\n",
    "    json.dump(split_data, fd)\n",
    "    fd.close()\n",
    "    print(\"... done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a95389b",
   "metadata": {},
   "source": [
    "Filter for traffic cones only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a7c0113",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'animal': 0, 'flat.driveable_surface': 1, 'human.pedestrian.adult': 2, 'human.pedestrian.child': 3, 'human.pedestrian.construction_worker': 4, 'human.pedestrian.personal_mobility': 5, 'human.pedestrian.police_officer': 6, 'human.pedestrian.stroller': 7, 'human.pedestrian.wheelchair': 8, 'movable_object.barrier': 9, 'movable_object.debris': 10, 'movable_object.pushable_pullable': 11, 'movable_object.trafficcone': 12, 'static_object.bicycle_rack': 13, 'vehicle.bicycle': 14, 'vehicle.bus.bendy': 15, 'vehicle.bus.rigid': 16, 'vehicle.car': 17, 'vehicle.construction': 18, 'vehicle.ego': 19, 'vehicle.emergency.ambulance': 20, 'vehicle.emergency.police': 21, 'vehicle.motorcycle': 22, 'vehicle.trailer': 23, 'vehicle.truck': 24}\n"
     ]
    }
   ],
   "source": [
    "print(category_id)\n",
    "traffic_cone_id = 12\n",
    "# movable_object.trafficcone: 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "db7efa66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filter detectron data for v1.0-train\n",
      "... done\n",
      "Filter detectron data for v1.0-val\n",
      "... done\n",
      "Filter detectron data for v1.0-test\n",
      "... done\n"
     ]
    }
   ],
   "source": [
    "splits = ['v1.0-train', 'v1.0-val', 'v1.0-test']\n",
    "for split in splits:\n",
    "    print(\"Filter detectron data for {}\".format(split))\n",
    "    path = os.path.join(os.path.join(dataroot, 'detectron_data'), split + '_detectron.json')\n",
    "    fd = open(path, 'r')\n",
    "    data = json.load(fd)\n",
    "    \n",
    "    filtered_data = []\n",
    "    for d in data:\n",
    "        filtered_anns = []\n",
    "        d_anns = d['annotations']\n",
    "            \n",
    "        for ann in d_anns:\n",
    "            if ann['category_id'] == traffic_cone_id:\n",
    "                ann['category_id'] = 0\n",
    "                filtered_anns.append(ann)\n",
    "        \n",
    "        if (len(filtered_anns) > 0):\n",
    "            d['annotations'] = filtered_anns\n",
    "            filtered_data.append(d)\n",
    "    \n",
    "    path = os.path.join(os.path.join(dataroot, 'detectron_data'), split + '_trafficcone_detectron.json')\n",
    "    fd2 = open(path, 'w+')\n",
    "    json.dump(filtered_data, fd2)\n",
    "    \n",
    "    fd.close()\n",
    "    fd2.close()\n",
    "    print(\"... done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ae5527d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_box_size(bbox):\n",
    "    return abs(bbox[0] - bbox[2]) * abs(bbox[1] - bbox[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1469e171",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filter detectron data for v1.0-train\n",
      "v1.0-train 67279\n",
      "v1.0-train small 157619\n",
      "v1.0-train medium 271410\n",
      "v1.0-train large 128686\n",
      "Filter detectron data for v1.0-val\n",
      "v1.0-val 16445\n",
      "v1.0-val small 40048\n",
      "v1.0-val medium 65547\n",
      "v1.0-val large 30479\n",
      "Filter detectron data for v1.0-test\n",
      "v1.0-test 9752\n",
      "v1.0-test small 0\n",
      "v1.0-test medium 0\n",
      "v1.0-test large 0\n"
     ]
    }
   ],
   "source": [
    "# Get counts\n",
    "splits = ['v1.0-train', 'v1.0-val', 'v1.0-test']\n",
    "for split in splits:\n",
    "    print(\"Filter detectron data for {}\".format(split))\n",
    "    path = os.path.join(os.path.join(dataroot, 'detectron_data'), split + '_detectron.json')\n",
    "    fd = open(path, 'r')\n",
    "    data = json.load(fd)\n",
    "    print(split, len(data))\n",
    "    small, medium, large = 0, 0, 0\n",
    "    for d in data:\n",
    "        for ann in d['annotations']:\n",
    "            if get_box_size(ann['bbox']) <= (32**2):\n",
    "                small += 1\n",
    "            elif get_box_size(ann['bbox']) <= (96**2):\n",
    "                medium += 1\n",
    "            else:\n",
    "                large += 1\n",
    "    print(split, \"small\", small)\n",
    "    print(split, \"medium\", medium)\n",
    "    print(split, \"large\", large)\n",
    "    fd.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4be16857",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr0/tma1/datasets/nuimages/detectron_data/v1.0-val_trafficcone_detectron.json\n",
      "4526\n"
     ]
    }
   ],
   "source": [
    "# Split validation data into validation and test\n",
    "path = os.path.join(os.path.join(dataroot, 'detectron_data'), 'v1.0-val_trafficcone_detectron.json')\n",
    "print(path)\n",
    "fd = open(path, 'r')\n",
    "data = json.load(fd)\n",
    "print(len(data))\n",
    "num_instances = len(data)\n",
    "indices = np.arange(num_instances)\n",
    "np.random.shuffle(indices)\n",
    "split_index = int(num_instances / 2)\n",
    "\n",
    "data_val = list(np.array(data)[indices[:split_index]])\n",
    "data_test = list(np.array(data)[indices[split_index:]])\n",
    "\n",
    "val_path = os.path.join(os.path.join(dataroot, 'detectron_data'), 'val_val_trafficcone_detectron.json')\n",
    "test_path = os.path.join(os.path.join(dataroot, 'detectron_data'), 'val_test_trafficcone_detectron.json')\n",
    "\n",
    "fd_val = open(val_path, 'w+')\n",
    "json.dump(data_val, fd_val)\n",
    "\n",
    "fd_test = open(test_path, 'w+')\n",
    "json.dump(data_test, fd_test)\n",
    "\n",
    "fd_val.close()\n",
    "fd_test.close()\n",
    "fd.close()\n",
    "\n",
    "#Done!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "63becf9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_path = os.path.join(os.path.join(dataroot, 'detectron_data'), 'val_val_trafficcone_detectron.json')\n",
    "fd = open(val_path, 'r')\n",
    "data = json.load(fd)\n",
    "fd.close()\n",
    "\n",
    "val_path_data = os.path.join(os.path.join(dataroot, 'detectron_data'), 'v1.0-val_val_trafficcone_detectron.json')\n",
    "val_data = dict()\n",
    "for d in data:\n",
    "    val_data[d['image_id']] = d\n",
    "\n",
    "fd = open(val_path_data, 'w+')\n",
    "json.dump(json.dumps(val_data), fd)\n",
    "\n",
    "test_path = os.path.join(os.path.join(dataroot, 'detectron_data'), 'val_test_trafficcone_detectron.json')\n",
    "fd = open(test_path, 'r')\n",
    "data = json.load(fd)\n",
    "fd.close()\n",
    "\n",
    "test_path_data = os.path.join(os.path.join(dataroot, 'detectron_data'), 'v1.0-val_test_trafficcone_detectron.json')\n",
    "test_data = dict()\n",
    "for d in data:\n",
    "    test_data[d['image_id']] = d\n",
    "\n",
    "fd = open(test_path_data, 'w+')\n",
    "json.dump(json.dumps(test_data), fd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7e83a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
